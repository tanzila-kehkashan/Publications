"""
UPD: Urdu Plagiarism Detection Tool
Authors: M. Hassaan Rafiq, Saad Razzaq, Tanzella Kehkashan
Paper: UPD: A Plagiarism Detection Tool for Urdu Language Documents
Journal: International Journal of Multidisciplinary Sciences and Engineering (2018)

This implementation provides plagiarism detection for Urdu language documents
using tokenization, stop word removal, trigram chunking, and absolute hashing.
"""

import re
from typing import List, Set, Tuple, Dict
import hashlib
from collections import Counter


class UrduPlagiarismDetector:
    """Main class for detecting plagiarism in Urdu documents"""

    def __init__(self):
        self.stop_words = self._load_urdu_stop_words()
        self.chunk_size = 3  # Trigram model (n=3)

    def _load_urdu_stop_words(self) -> Set[str]:
        """Load common Urdu stop words"""
        # Common Urdu stop words
        stop_words = {
            'Ú©Ø§', 'Ú©ÛŒ', 'Ú©Û’', 'Ú©Ùˆ', 'Ù†Û’', 'Ù…ÛŒÚº', 'Ø³Û’', 'Ù¾Ø±',
            'Ø§ÙˆØ±', 'ÛŒÛ', 'ÙˆÛ', 'ÛÛ’', 'ÛÛŒÚº', 'ØªÚ¾Ø§', 'ØªÚ¾ÛŒ', 'ØªÚ¾Û’',
            'Ú¯Ø§', 'Ú¯ÛŒ', 'Ú¯Û’', 'ÛÙˆ', 'ÛÙˆÚº', 'ÛÙˆØ¦ÛŒ', 'ÛÙˆØ¦Û’', 'ÛÙˆØ§',
            'Ú©Ø±', 'Ú©Ø±Ù†Û’', 'Ú©ÛŒØ§', 'Ú©ÛŒÙˆÚº', 'Ú©Û', 'Ø¬Ùˆ', 'Ø¬Ø¨', 'Ø¬ÛØ§Úº',
            'Ø§ÛŒÚ©', 'Ø¯Ùˆ', 'ØªÛŒÙ†', 'ÛŒØ§', 'Ø¨Ú¾ÛŒ', 'Ù†ÛÛŒÚº', 'Ø§Ø³', 'Ø§Ù†',
            'ØªÙˆ', 'ÛÛŒ', 'Ø§Ø¨Ú¾ÛŒ', 'Ù„ÛŒÚ©Ù†', 'Ù…Ú¯Ø±', 'Ø§Ú¯Ø±', 'ØªØ§Ú©Û'
        }
        return stop_words

    def tokenize(self, text: str) -> List[str]:
        """
        Tokenize Urdu text into words

        Args:
            text: Urdu text string

        Returns:
            List of tokens (words)
        """
        # Remove punctuation and extra whitespace
        text = re.sub(r'[Û”Ø›ØŒØÙªÙ«Ù¬Û]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()

        # Split into words
        tokens = text.split()

        return tokens

    def remove_stop_words(self, tokens: List[str]) -> List[str]:
        """
        Remove Urdu stop words from token list

        Args:
            tokens: List of word tokens

        Returns:
            Filtered list without stop words
        """
        filtered_tokens = [
            token for token in tokens
            if token not in self.stop_words
        ]
        return filtered_tokens

    def create_trigrams(self, tokens: List[str]) -> List[Tuple[str, str, str]]:
        """
        Create trigrams (3-word chunks) from token list

        Args:
            tokens: List of filtered tokens

        Returns:
            List of trigrams as tuples
        """
        if len(tokens) < self.chunk_size:
            return [(tokens[0], tokens[1] if len(tokens) > 1 else '',
                    tokens[2] if len(tokens) > 2 else '')]

        trigrams = []
        for i in range(len(tokens) - self.chunk_size + 1):
            trigram = tuple(tokens[i:i + self.chunk_size])
            trigrams.append(trigram)

        return trigrams

    def absolute_hash(self, trigram: Tuple[str, str, str]) -> int:
        """
        Compute absolute hash value for a trigram
        Position of character matters in this hash function

        Args:
            trigram: Tuple of three words

        Returns:
            Integer hash value
        """
        hash_value = 0
        position_multiplier = 1

        for word in trigram:
            for char in word:
                # Get Unicode code point of character
                char_value = ord(char)
                # Multiply by position for absolute hashing
                hash_value += char_value * position_multiplier
                position_multiplier += 1

        return hash_value

    def compute_fingerprint(self, trigrams: List[Tuple[str, str, str]]) -> Set[int]:
        """
        Compute fingerprint set from trigrams using absolute hashing

        Args:
            trigrams: List of trigrams

        Returns:
            Set of hash values (fingerprint)
        """
        fingerprint = set()
        for trigram in trigrams:
            hash_val = self.absolute_hash(trigram)
            fingerprint.add(hash_val)

        return fingerprint

    def calculate_resemblance(self, fp_a: Set[int], fp_b: Set[int]) -> float:
        """
        Calculate resemblance measure R between two fingerprints

        R = |S(A) âˆ© S(B)| / |S(A) âˆª S(B)|

        Args:
            fp_a: Fingerprint set of document A
            fp_b: Fingerprint set of document B

        Returns:
            Resemblance value between 0 and 1
        """
        if len(fp_a) == 0 and len(fp_b) == 0:
            return 1.0  # Both empty, considered identical

        if len(fp_a) == 0 or len(fp_b) == 0:
            return 0.0  # One empty, no similarity

        # Intersection: matched trigrams
        matched = fp_a & fp_b
        M = len(matched)

        # Union: total unique trigrams
        total = fp_a | fp_b
        N = len(total)

        # Resemblance measure
        R = M / N if N > 0 else 0.0

        return R

    def detect_plagiarism(self, doc1: str, doc2: str) -> Dict:
        """
        Main method to detect plagiarism between two Urdu documents

        Args:
            doc1: First Urdu document text
            doc2: Second Urdu document text

        Returns:
            Dictionary with plagiarism detection results
        """
        # Step 1: Tokenization
        tokens1 = self.tokenize(doc1)
        tokens2 = self.tokenize(doc2)

        # Step 2: Stop word removal
        filtered1 = self.remove_stop_words(tokens1)
        filtered2 = self.remove_stop_words(tokens2)

        # Step 3: Create trigrams
        trigrams1 = self.create_trigrams(filtered1)
        trigrams2 = self.create_trigrams(filtered2)

        # Step 4: Compute fingerprints using absolute hashing
        fingerprint1 = self.compute_fingerprint(trigrams1)
        fingerprint2 = self.compute_fingerprint(trigrams2)

        # Step 5: Calculate resemblance
        similarity = self.calculate_resemblance(fingerprint1, fingerprint2)

        # Prepare results
        results = {
            'similarity_percentage': similarity * 100,
            'doc1_tokens': len(tokens1),
            'doc2_tokens': len(tokens2),
            'doc1_filtered_tokens': len(filtered1),
            'doc2_filtered_tokens': len(filtered2),
            'doc1_trigrams': len(trigrams1),
            'doc2_trigrams': len(trigrams2),
            'doc1_fingerprint_size': len(fingerprint1),
            'doc2_fingerprint_size': len(fingerprint2),
            'matched_trigrams': len(fingerprint1 & fingerprint2),
            'total_unique_trigrams': len(fingerprint1 | fingerprint2)
        }

        return results

    def load_document(self, file_path: str) -> str:
        """
        Load Urdu document from file

        Args:
            file_path: Path to text file

        Returns:
            Document content as string
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
            return content
        except FileNotFoundError:
            raise FileNotFoundError(f"Document not found: {file_path}")
        except Exception as e:
            raise Exception(f"Error loading document: {str(e)}")


def print_results(results: Dict):
    """Pretty print plagiarism detection results"""
    print("\n" + "=" * 60)
    print("URDU PLAGIARISM DETECTION RESULTS")
    print("=" * 60)

    print(f"\n{'Similarity:':<30} {results['similarity_percentage']:.2f}%")

    print(f"\n{'DOCUMENT 1 STATISTICS':^60}")
    print("-" * 60)
    print(f"{'Total Tokens:':<30} {results['doc1_tokens']}")
    print(f"{'Filtered Tokens:':<30} {results['doc1_filtered_tokens']}")
    print(f"{'Trigrams Generated:':<30} {results['doc1_trigrams']}")
    print(f"{'Fingerprint Size:':<30} {results['doc1_fingerprint_size']}")

    print(f"\n{'DOCUMENT 2 STATISTICS':^60}")
    print("-" * 60)
    print(f"{'Total Tokens:':<30} {results['doc2_tokens']}")
    print(f"{'Filtered Tokens:':<30} {results['doc2_filtered_tokens']}")
    print(f"{'Trigrams Generated:':<30} {results['doc2_trigrams']}")
    print(f"{'Fingerprint Size:':<30} {results['doc2_fingerprint_size']}")

    print(f"\n{'COMPARISON METRICS':^60}")
    print("-" * 60)
    print(f"{'Matched Trigrams (M):':<30} {results['matched_trigrams']}")
    print(f"{'Total Unique Trigrams (N):':<30} {results['total_unique_trigrams']}")

    print("\n" + "=" * 60)

    # Interpretation
    similarity = results['similarity_percentage']
    if similarity >= 70:
        print("âš ï¸  HIGH SIMILARITY - Potential plagiarism detected!")
    elif similarity >= 30:
        print("âš¡ MODERATE SIMILARITY - Review recommended")
    else:
        print("âœ… LOW SIMILARITY - Documents appear to be original")

    print("=" * 60 + "\n")


def main():
    """Example usage demonstration"""

    print("UPD: Urdu Plagiarism Detection Tool")
    print("=" * 60)

    # Initialize detector
    detector = UrduPlagiarismDetector()

    # Example Urdu documents
    doc1 = """
    Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¬Ù†ÙˆØ¨ÛŒ Ø§ÛŒØ´ÛŒØ§ Ù…ÛŒÚº ÙˆØ§Ù‚Ø¹ Ø§ÛŒÚ© Ø®ÙˆØ¨ØµÙˆØ±Øª Ù…Ù„Ú© ÛÛ’Û” ÛŒÛØ§Úº Ú©ÛŒ Ø«Ù‚Ø§ÙØª Ø¨ÛØª
    Ù…ØªÙ†ÙˆØ¹ ÛÛ’ Ø§ÙˆØ± Ù„ÙˆÚ¯ Ù…ÛÙ…Ø§Ù† Ù†ÙˆØ§Ø² ÛÛŒÚºÛ” Ù¾Ø§Ú©Ø³ØªØ§Ù† Ù…ÛŒÚº Ø¨ÛØª Ø³Û’ ØªØ§Ø±ÛŒØ®ÛŒ Ù…Ù‚Ø§Ù…Ø§Øª
    Ù…ÙˆØ¬ÙˆØ¯ ÛÛŒÚºÛ” ÛŒÛØ§Úº Ú©ÛŒ Ù‚Ø¯Ø±ØªÛŒ Ø®ÙˆØ¨ØµÙˆØ±ØªÛŒ Ø¯Ù†ÛŒØ§ Ø¨Ú¾Ø± Ù…ÛŒÚº Ù…Ø´ÛÙˆØ± ÛÛ’Û”
    """

    doc2 = """
    Ù¾Ø§Ú©Ø³ØªØ§Ù† Ø¬Ù†ÙˆØ¨ÛŒ Ø§ÛŒØ´ÛŒØ§ Ù…ÛŒÚº ÙˆØ§Ù‚Ø¹ Ø§ÛŒÚ© Ù¾Ø±ÙØ¶Ø§ Ù…Ù„Ú© ÛÛ’Û” ÛŒÛØ§Úº Ú©ÛŒ Ø±ÙˆØ§ÛŒØ§Øª Ø¨ÛØª
    Ù…ØªÙ†ÙˆØ¹ ÛÛŒÚº Ø§ÙˆØ± Ø´ÛØ±ÛŒ Ù…ÛÙ…Ø§Ù† Ù†ÙˆØ§Ø² ÛÛŒÚºÛ” Ù¾Ø§Ú©Ø³ØªØ§Ù† Ù…ÛŒÚº Ú©Ø¦ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ Ù…Ù‚Ø§Ù…Ø§Øª
    Ù…Ù„ØªÛ’ ÛÛŒÚºÛ” ÛŒÛØ§Úº Ú©ÛŒ ÙØ·Ø±ÛŒ Ø­Ø³Ù† Ø¹Ø§Ù„Ù…ÛŒ Ø³Ø·Ø­ Ù¾Ø± Ù…Ø¹Ø±ÙˆÙ ÛÛ’Û”
    """

    doc3 = """
    ØªØ¹Ù„ÛŒÙ… ÛØ± Ù…Ø¹Ø§Ø´Ø±Û’ Ú©ÛŒ ØªØ±Ù‚ÛŒ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ ÛÛ’Û” Ø§Ú†Ú¾ÛŒ ØªØ¹Ù„ÛŒÙ… Ø³Û’ Ù†ÙˆØ¬ÙˆØ§Ù† Ù†Ø³Ù„ Ú©Ùˆ
    Ø¨ÛØªØ± Ù…Ø³ØªÙ‚Ø¨Ù„ Ù…Ù„ Ø³Ú©ØªØ§ ÛÛ’Û” ÛÙ…ÛŒÚº Ø§Ù¾Ù†Û’ ØªØ¹Ù„ÛŒÙ…ÛŒ Ù†Ø¸Ø§Ù… Ú©Ùˆ Ù…Ø²ÛŒØ¯ Ø¨ÛØªØ± Ø¨Ù†Ø§Ù†Û’
    Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÛ’Û” Ú©ÛŒÙˆÙ†Ú©Û Ø¹Ù„Ù… ÛÛŒ Ù‚ÙˆÙ…ÙˆÚº Ú©ÛŒ ØªØ±Ù‚ÛŒ Ú©Ø§ Ø±Ø§Ø² ÛÛ’Û”
    """

    # Test Case 1: High similarity
    print("\nğŸ“Š Test Case 1: Comparing similar documents")
    results1 = detector.detect_plagiarism(doc1, doc2)
    print_results(results1)

    # Test Case 2: Low similarity
    print("\nğŸ“Š Test Case 2: Comparing different documents")
    results2 = detector.detect_plagiarism(doc1, doc3)
    print_results(results2)

    # Test Case 3: Identical documents
    print("\nğŸ“Š Test Case 3: Comparing identical documents")
    results3 = detector.detect_plagiarism(doc1, doc1)
    print_results(results3)


if __name__ == "__main__":
    main()
